## Parameters for GRPO

model_name: "Qwen2-0.5B-Instruct" ## "Qwen2-0.5B-Instruct", "SmolLM2-135M-Instruct"
checkpoint: 7000

reward_fcts: ["risk_seeking"] # "risk_averse", "risk_seeking"

learning_rate: 0.000001
patience: 5
per_device_train_batch_size: 8
gradient_accumulation_steps: 4
num_generations: 8
eval_steps: 500
save_steps: 500
save_total_limit: 5
logging_steps: 25

lora_r: 16
lora_alpha: 64
lora_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"]
lora_dropout: 0.05
